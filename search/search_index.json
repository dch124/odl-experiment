{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Notes on MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Notes on MkDocs"},{"location":"#notes-on-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Notes on MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"unit%201/backpropagation/","text":"Backpropagation The functions we deal with in deep neural networks have many variables and a deep compositional structure. To compute gradients efficiently, we represent the evaluation of a given function as a computation graph . For example, the evaluation of the function e=(a+b)*(b+1) e=(a+b)*(b+1) at a=2, b=3 a=2, b=3 can be represented as follows: ![DAG for the example above] This shows how the values of the input variables are mapped through intermediate steps to produce an output. We will use this graph to compute gradients in an efficient way. First, consider the special case of a function that is a composition of two elementary functions each taking a single argument, as in the following example: ![three-node DAG] The derivatives between neighbouring variables are shown on the edges that link them. By virtue of the chain rule , the gradient of z z with respect to x x is simply the product of the two intermediate derivatives: \\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial x} \\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial x} . This value is shown against the node for x x . An intuitive example of this is due to George Simmons [1]: \"if a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 \u00d7 4 = 8 times as fast as the man.\" [2] For compositions of more than two elementary functions, repeated application of the chain rule means that the derivative of the output variable with respect to any other variable is simply the product of the derivatives on the intermediate edges, as shown below: ![five node DAG] Backpropagation computes all of these partial derivatives efficiently by propagating the gradient backwards from the output, each time multiplying by the next edge derivative in the chain. The iteration is initialised by placing a gradient of 1 at the output node (i.e. \\frac{\\partial e}{\\partial e}=1 \\frac{\\partial e}{\\partial e}=1 ). In general, the computation graph may contain functions with more than one argument, as in the graph at the top. In this case the chain rule takes a more general form such that the gradient at a variable becomes the sum of the products of derivatives along all paths between the target variable and the output. Again, backpropagation computes all of these gradients efficiently by working backwards from the output, compounding the product of derivatives along every branch of the DAG and accumlating (adding) gradients at nodes that have multiple paths to the output. This backpropagation of gradients is illustrated below: ![example above augmented with gradients and arrows] Finally, consider the following DAG. One of the m m routes from x_2 x_2 goes via y_3 y_3 and onward to e. The product \\frac{\\partial y_3}{\\partial x_2} \\frac{\\partial e} {\\partial y_3} \\frac{\\partial y_3}{\\partial x_2} \\frac{\\partial e} {\\partial y_3} is the corresponding contribution to the gradient at x_2 x_2 . If we write down the whole computation for {x_i} {x_i} , summing over all m m paths we get: x_i = \\sum_{j=1}^{m} \\frac{\\partial y_j}{\\partial x_i}\\frac{\\partial e}{\\partial y_j} x_i = \\sum_{j=1}^{m} \\frac{\\partial y_j}{\\partial x_i}\\frac{\\partial e}{\\partial y_j} If we wrap up the x_i x_i and y_j y_j into a pair of vectors \\vec{x}, \\vec{y} \\vec{x}, \\vec{y} , we can express the whole computation as: [1] George F. Simmons, Calculus with Analytic Geometry (1985), p93","title":"Backpropagation"},{"location":"unit%201/backpropagation/#backpropagation","text":"The functions we deal with in deep neural networks have many variables and a deep compositional structure. To compute gradients efficiently, we represent the evaluation of a given function as a computation graph . For example, the evaluation of the function e=(a+b)*(b+1) e=(a+b)*(b+1) at a=2, b=3 a=2, b=3 can be represented as follows: ![DAG for the example above] This shows how the values of the input variables are mapped through intermediate steps to produce an output. We will use this graph to compute gradients in an efficient way. First, consider the special case of a function that is a composition of two elementary functions each taking a single argument, as in the following example: ![three-node DAG] The derivatives between neighbouring variables are shown on the edges that link them. By virtue of the chain rule , the gradient of z z with respect to x x is simply the product of the two intermediate derivatives: \\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial x} \\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial x} . This value is shown against the node for x x . An intuitive example of this is due to George Simmons [1]: \"if a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 \u00d7 4 = 8 times as fast as the man.\" [2] For compositions of more than two elementary functions, repeated application of the chain rule means that the derivative of the output variable with respect to any other variable is simply the product of the derivatives on the intermediate edges, as shown below: ![five node DAG] Backpropagation computes all of these partial derivatives efficiently by propagating the gradient backwards from the output, each time multiplying by the next edge derivative in the chain. The iteration is initialised by placing a gradient of 1 at the output node (i.e. \\frac{\\partial e}{\\partial e}=1 \\frac{\\partial e}{\\partial e}=1 ). In general, the computation graph may contain functions with more than one argument, as in the graph at the top. In this case the chain rule takes a more general form such that the gradient at a variable becomes the sum of the products of derivatives along all paths between the target variable and the output. Again, backpropagation computes all of these gradients efficiently by working backwards from the output, compounding the product of derivatives along every branch of the DAG and accumlating (adding) gradients at nodes that have multiple paths to the output. This backpropagation of gradients is illustrated below: ![example above augmented with gradients and arrows] Finally, consider the following DAG. One of the m m routes from x_2 x_2 goes via y_3 y_3 and onward to e. The product \\frac{\\partial y_3}{\\partial x_2} \\frac{\\partial e} {\\partial y_3} \\frac{\\partial y_3}{\\partial x_2} \\frac{\\partial e} {\\partial y_3} is the corresponding contribution to the gradient at x_2 x_2 . If we write down the whole computation for {x_i} {x_i} , summing over all m m paths we get: x_i = \\sum_{j=1}^{m} \\frac{\\partial y_j}{\\partial x_i}\\frac{\\partial e}{\\partial y_j} x_i = \\sum_{j=1}^{m} \\frac{\\partial y_j}{\\partial x_i}\\frac{\\partial e}{\\partial y_j} If we wrap up the x_i x_i and y_j y_j into a pair of vectors \\vec{x}, \\vec{y} \\vec{x}, \\vec{y} , we can express the whole computation as: [1] George F. Simmons, Calculus with Analytic Geometry (1985), p93","title":"Backpropagation"},{"location":"unit%201/optimisation/","text":"Optimisation Optmisation is fundamental to machine learning. Here we look at the general problem of optimisation and later apply it to machine learning. The problem In its simplest form, the optimsation problem can be expressed as follows. Given some function f:\\R\\rightarrow\\R f:\\R\\rightarrow\\R we seek x^* x^* , the value of x x that gives the minimum output from the function: x^*=\\underset{x} {\\operatorname{argmin}}f(x) x^*=\\underset{x} {\\operatorname{argmin}}f(x) For functions with n-dimensional input and a scalar output f:\\R^n\\rightarrow\\R f:\\R^n\\rightarrow\\R we seek \\myvec{x}^* \\myvec{x}^* , the value of \\myvec{x} \\myvec{x} that gives the minimum output from the function: \\myvec{x}^*=\\underset{\\myvec{x\\in\\R^n}} {\\operatorname{argmin}}f(\\myvec{x}) \\myvec{x}^*=\\underset{\\myvec{x\\in\\R^n}} {\\operatorname{argmin}}f(\\myvec{x}) Gradient descent Gradient descent is a general approach for finding the minimum of a function. The idea is to start with some random \\myvec{x} \\myvec{x} and make repeated small step-changes to \\myvec{x} \\myvec{x} in the direction of steepest descent of the output f(\\myvec{x}) f(\\myvec{x}) . The following diagram illustrates this idea for n=2 n=2 . To do this we need to be able to work out the gradient of the surface at a given point. This amounts to seeing how the output changes with infinitesimally small changes in each of the components of \\myvec{x} \\myvec{x} - the so-called gradient vector : \\nabla f(\\myvec{x}) = \\begin{bmatrix} \\frac {\\partial f}{\\partial x_1} (\\myvec{x})\\\\ \\vdots \\\\ \\frac {\\partial f}{\\partial x_n} (\\myvec{x}) \\end{bmatrix} \\nabla f(\\myvec{x}) = \\begin{bmatrix} \\frac {\\partial f}{\\partial x_1} (\\myvec{x})\\\\ \\vdots \\\\ \\frac {\\partial f}{\\partial x_n} (\\myvec{x}) \\end{bmatrix} For example, in 2-D suppose f(x,y)=3x^2+xy f(x,y)=3x^2+xy The gradient vector is then given by \\nabla f(x,y)=\\begin{bmatrix} 6x \\\\ x \\end {bmatrix} \\nabla f(x,y)=\\begin{bmatrix} 6x \\\\ x \\end {bmatrix} Thus, the gradient myvector at (3,4) (3,4) is \\begin{bmatrix} 18 \\\\ 3 \\end {bmatrix} \\begin{bmatrix} 18 \\\\ 3 \\end {bmatrix} . This vector is in the direction of the steepest slope and has magnitude equal to the slope. This is the direction in which we should move to get closer to the summit (maximum value). To reach the minimum we move in the opposite direction. Thus, we make a small step from \\myvec{x} \\myvec{x} to a new point \\myvec{x'} \\myvec{x'} defined as follows: \\myvec{x'}=\\myvec{x}-\\eta \\nabla f(\\myvec{x}) \\myvec{x'}=\\myvec{x}-\\eta \\nabla f(\\myvec{x}) \\eta \\eta simply scales the step size and is generally a small number (e.g. \\eta=0.002 \\eta=0.002 ) Notice that the displacement is subtracted so that we move downhill.","title":"Optimisation"},{"location":"unit%201/optimisation/#optimisation","text":"Optmisation is fundamental to machine learning. Here we look at the general problem of optimisation and later apply it to machine learning.","title":"Optimisation"},{"location":"unit%201/optimisation/#the-problem","text":"In its simplest form, the optimsation problem can be expressed as follows. Given some function f:\\R\\rightarrow\\R f:\\R\\rightarrow\\R we seek x^* x^* , the value of x x that gives the minimum output from the function: x^*=\\underset{x} {\\operatorname{argmin}}f(x) x^*=\\underset{x} {\\operatorname{argmin}}f(x) For functions with n-dimensional input and a scalar output f:\\R^n\\rightarrow\\R f:\\R^n\\rightarrow\\R we seek \\myvec{x}^* \\myvec{x}^* , the value of \\myvec{x} \\myvec{x} that gives the minimum output from the function: \\myvec{x}^*=\\underset{\\myvec{x\\in\\R^n}} {\\operatorname{argmin}}f(\\myvec{x}) \\myvec{x}^*=\\underset{\\myvec{x\\in\\R^n}} {\\operatorname{argmin}}f(\\myvec{x})","title":"The problem"},{"location":"unit%201/optimisation/#gradient-descent","text":"Gradient descent is a general approach for finding the minimum of a function. The idea is to start with some random \\myvec{x} \\myvec{x} and make repeated small step-changes to \\myvec{x} \\myvec{x} in the direction of steepest descent of the output f(\\myvec{x}) f(\\myvec{x}) . The following diagram illustrates this idea for n=2 n=2 . To do this we need to be able to work out the gradient of the surface at a given point. This amounts to seeing how the output changes with infinitesimally small changes in each of the components of \\myvec{x} \\myvec{x} - the so-called gradient vector : \\nabla f(\\myvec{x}) = \\begin{bmatrix} \\frac {\\partial f}{\\partial x_1} (\\myvec{x})\\\\ \\vdots \\\\ \\frac {\\partial f}{\\partial x_n} (\\myvec{x}) \\end{bmatrix} \\nabla f(\\myvec{x}) = \\begin{bmatrix} \\frac {\\partial f}{\\partial x_1} (\\myvec{x})\\\\ \\vdots \\\\ \\frac {\\partial f}{\\partial x_n} (\\myvec{x}) \\end{bmatrix} For example, in 2-D suppose f(x,y)=3x^2+xy f(x,y)=3x^2+xy The gradient vector is then given by \\nabla f(x,y)=\\begin{bmatrix} 6x \\\\ x \\end {bmatrix} \\nabla f(x,y)=\\begin{bmatrix} 6x \\\\ x \\end {bmatrix} Thus, the gradient myvector at (3,4) (3,4) is \\begin{bmatrix} 18 \\\\ 3 \\end {bmatrix} \\begin{bmatrix} 18 \\\\ 3 \\end {bmatrix} . This vector is in the direction of the steepest slope and has magnitude equal to the slope. This is the direction in which we should move to get closer to the summit (maximum value). To reach the minimum we move in the opposite direction. Thus, we make a small step from \\myvec{x} \\myvec{x} to a new point \\myvec{x'} \\myvec{x'} defined as follows: \\myvec{x'}=\\myvec{x}-\\eta \\nabla f(\\myvec{x}) \\myvec{x'}=\\myvec{x}-\\eta \\nabla f(\\myvec{x}) \\eta \\eta simply scales the step size and is generally a small number (e.g. \\eta=0.002 \\eta=0.002 ) Notice that the displacement is subtracted so that we move downhill.","title":"Gradient descent"},{"location":"unit%201/pytorch/","text":"PyTorch","title":"Pytorch"},{"location":"unit%201/pytorch/#pytorch","text":"","title":"PyTorch"}]}